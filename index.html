<!DOCTYPE html>
<!-- saved from url=(0033)https://leiyi420.github.io/HierarchicalEmoTTS/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Cross-speaker Emotion Transfer for Expressive Speech Synthesis through Information Perturbation</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="TODO: title">
<meta property="og:locale" content="en_US">
<link rel="canonical" href="https://leiyi420.github.io/CSEmoTransfer">
<meta property="og:url" content="https://leiyi420.github.io/CSEmoTransfer">
<meta name="twitter:card" content="summary">
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="style.css">
  </head>
  <body data-new-gr-c-s-check-loaded="14.1001.0" data-gr-ext-installed="">
    <section class="page-header">
    <!-- <h1 class="project-name">Demo PAGE</h1> -->
    <!-- <h2 class="project-tagline"></h2> -->
      
      
    </section>

<section class="main-content">
      <h1 id=""><center>Cross-speaker Emotion Transfer for Expressive Speech Synthesis through Information Perturbation</center></h1>

<center> Yi Lei, Shan Yang, Xinfa Zhu, Qicong Xie, Lei Xie, Dan Su  </center>
<center> Northwestern Polytechnical University </center>
<center> Tencent AI Lab </center>

<h2>0. Contents</h2>
<ol>
  <li><a href="#abstract">Abstract</a></li>


</ol>

<br><br>
<h2 id="abstract">1. Abstract<a name="abstract"></a></h2>
<p> Cross-speaker emotion transfer is an effective way to produce expressive speech for neutral target speakers, which doesn't require emotional training data of target speakers. Since the emotion and timbre of the source speaker are heavily entangled, existing approaches often struggle to trade off the speaker similarity and the emotion expressions.
 In this paper, we propose to disentangle the timbre and the emotion of speech through information perturbation to conduct cross-speaker emotion transfer, which effectively learns the emotion expressions of the source speaker and maintains the timbre of the target neutral speakers. Specifically, we perturb the timbre and emotion information (e.g., formant and pitch) of source speech separately to obtain and model the emotion- and timbre-independent signals, based on which the proposed model could further produce emotional speech in the timbre of target speakers. Experiment results demonstrate that the proposed approach significantly outperforms the baseline models in terms of naturalness and similarity, indicating the effectiveness of information perturbation for cross-speaker emotion transfer.
 </p>
<br><br>


</body></html>
